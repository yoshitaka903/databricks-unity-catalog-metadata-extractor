{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4169f39e-c56c-43ad-a1fa-14e7f12e861e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Unity Catalog メタデータ抽出ノートブック\n",
    "このノートブックはUnity Catalogから既存テーブルのメタデータを抽出・出力します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28a54489-b7f8-4ddc-a5ac-e85361cf58ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. 初期設定・パラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62263696-e77f-492b-bff5-1a517c676cf0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ライブラリのインポート"
    }
   },
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from typing import Dict, Any, List, Tuple, Set, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "666aaed8-a31b-4732-9d62-885605a61218",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "カタログ選択とスキーマ一覧の取得"
    }
   },
   "outputs": [],
   "source": [
    "# 対象のカタログを指定\n",
    "catalog = \"samples\"\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "\n",
    "# カタログ配下のスキーマ一覧を取得\n",
    "schemas_sql = f\"\"\"\n",
    "SELECT \n",
    "    catalog_name, \n",
    "    schema_name, \n",
    "    schema_owner, \n",
    "    created, \n",
    "    last_altered \n",
    "FROM {catalog}.information_schema.schemata\n",
    "WHERE 1=1\n",
    "  AND schema_name <> 'information_schema'\n",
    "ORDER BY catalog_name, schema_name\n",
    "\"\"\"\n",
    "\n",
    "schemas_df = spark.sql(schemas_sql)\n",
    "display(schemas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0071c791-fec4-464a-8d9e-13225f8c46d1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "スキーマ名のリスト化"
    }
   },
   "outputs": [],
   "source": [
    "# カタログ配下のスキーマ一覧を取得・確認\n",
    "schema_list = [row[\"schema_name\"] for row in schemas_df.select(\"schema_name\").collect()]\n",
    "schema_list\n",
    "# 取得するスキーマを絞りたい場合はここで schema_list を絞り込むこと"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "583ac399-148f-41c3-87c8-ae8d1581fb9a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CONFIG"
    }
   },
   "outputs": [],
   "source": [
    "# 統合設定（重複削除・一元管理）\n",
    "CONFIG = {\n",
    "    \"target_catalog\": catalog,\n",
    "    \"include_schemas\": schema_list,\n",
    "    \"output_catalog\": \"ops\",\n",
    "    \"output_schema\": catalog,\n",
    "    \"exclude_patterns\": ['^__', '^event_log_'],\n",
    "    \"table_types\": ['MANAGED', 'EXTERNAL', 'VIEW'],\n",
    "    \"retention_days\": 180,\n",
    "    \"max_parallel_workers\": 4,\n",
    "    \n",
    "}\n",
    "\n",
    "print(f\"対象カタログ: {CONFIG['target_catalog']}\")\n",
    "print(f\"対象スキーマ: {CONFIG['include_schemas']}\")\n",
    "print(f\"実行時刻: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22333380-aee5-4100-8078-1e492f7cbebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. 共通処理・ユーティリティ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "168508fd-0923-4e50-96d0-daae3ce96eff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ユーティリティ関数"
    }
   },
   "outputs": [],
   "source": [
    "def build_filter_conditions(config):\n",
    "    \"\"\"共通フィルタ条件を生成（重複削除）\"\"\"\n",
    "    schema_filter = \"', '\".join(config[\"include_schemas\"])\n",
    "    exclude_where = \" AND \".join([f\"table_name NOT RLIKE '{p}'\" for p in config[\"exclude_patterns\"]])\n",
    "    table_types = \"', '\".join(config[\"table_types\"])\n",
    "    return schema_filter, exclude_where, table_types\n",
    "\n",
    "def q(identifier: str) -> str:\n",
    "    \"\"\"Spark SQL用にバッククォートでエスケープ\"\"\"\n",
    "    return f\"`{identifier.replace('`', '``')}`\"\n",
    "\n",
    "def fqname(catalog: str, schema: str, table: str) -> str:\n",
    "    \"\"\"完全修飾名を生成\"\"\"\n",
    "    return f\"{q(catalog)}.{q(schema)}.{q(table)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbc2dd94-6d36-4988-a8a3-fe2f5dede80b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. テーブル基本情報取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d4db552-0fb3-45f3-93a8-7db1c3620353",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "テーブル一覧の抽出とビュー作成"
    }
   },
   "outputs": [],
   "source": [
    "# 共通フィルタ条件を使用\n",
    "schema_filter, exclude_where, table_types = build_filter_conditions(CONFIG)\n",
    "\n",
    "tables_sql = f\"\"\"\n",
    "SELECT \n",
    "    table_catalog,\n",
    "    table_schema,\n",
    "    table_name,\n",
    "    table_type,\n",
    "    table_owner,\n",
    "    created,\n",
    "    last_altered\n",
    "FROM {CONFIG['target_catalog']}.information_schema.tables \n",
    "WHERE 1=1\n",
    "  AND table_schema IN ('{schema_filter}')\n",
    "  AND table_type IN ('{table_types}')\n",
    "  AND {exclude_where}\n",
    "ORDER BY table_catalog, table_schema, table_name\n",
    "\"\"\"\n",
    "\n",
    "tables_df = spark.sql(tables_sql)\n",
    "tables_df.createOrReplaceTempView(\"base_tables\")\n",
    "\n",
    "table_count = tables_df.count()\n",
    "print(f\"取得テーブル数: {table_count}\")\n",
    "\n",
    "if table_count > 0:\n",
    "    display(tables_df.limit(5))\n",
    "else:\n",
    "    print(\"対象テーブルはありません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b55393c-a529-4cd1-83f5-183f553247b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. カラム基本情報の取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d76eb08-fd81-43d0-85e2-612ef0706df4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "カラム情報の取得"
    }
   },
   "outputs": [],
   "source": [
    "# カラム基本情報と制約フラグの統合取得\n",
    "exclude_where_tc = \" AND \".join([f\"tc.table_name NOT RLIKE '{p}'\" for p in CONFIG[\"exclude_patterns\"]])\n",
    "integrated_sql = f\"\"\"\n",
    "WITH base_columns AS (\n",
    "    SELECT \n",
    "        table_catalog,\n",
    "        table_schema,\n",
    "        table_name,\n",
    "        column_name,\n",
    "        ordinal_position + 1 as ordinal_position,\n",
    "        data_type,\n",
    "        is_nullable,\n",
    "        column_default,\n",
    "        numeric_precision,\n",
    "        numeric_scale\n",
    "    FROM {CONFIG['target_catalog']}.information_schema.columns \n",
    "    WHERE table_schema IN ('{schema_filter}')\n",
    "      AND {exclude_where}\n",
    "),\n",
    "constraint_flags AS (\n",
    "    SELECT \n",
    "        kcu.table_schema, \n",
    "        kcu.table_name, \n",
    "        kcu.column_name,\n",
    "        MAX(CASE WHEN tc.constraint_type = 'PRIMARY KEY' THEN 1 ELSE 0 END) as is_pk,\n",
    "        MAX(CASE WHEN tc.constraint_type = 'FOREIGN KEY' THEN 1 ELSE 0 END) as is_fk\n",
    "    FROM {CONFIG['target_catalog']}.information_schema.table_constraints tc\n",
    "    JOIN {CONFIG['target_catalog']}.information_schema.key_column_usage kcu \n",
    "        ON tc.constraint_catalog = kcu.constraint_catalog \n",
    "        AND tc.constraint_schema = kcu.constraint_schema\n",
    "        AND tc.constraint_name = kcu.constraint_name\n",
    "    WHERE tc.constraint_type IN ('PRIMARY KEY', 'FOREIGN KEY')\n",
    "      AND tc.constraint_schema IN ('{schema_filter}')\n",
    "      AND {exclude_where_tc}\n",
    "    GROUP BY kcu.table_schema, kcu.table_name, kcu.column_name\n",
    ")\n",
    "SELECT \n",
    "    c.*,\n",
    "    COALESCE(cf.is_pk, 0) as is_pk,\n",
    "    COALESCE(cf.is_fk, 0) as is_fk\n",
    "FROM base_columns c\n",
    "LEFT JOIN constraint_flags cf \n",
    "    ON c.table_schema = cf.table_schema \n",
    "    AND c.table_name = cf.table_name \n",
    "    AND LOWER(c.column_name) = LOWER(cf.column_name)\n",
    "ORDER BY c.table_catalog, c.table_schema, c.table_name, c.ordinal_position\n",
    "\"\"\"\n",
    "\n",
    "integrated_df = spark.sql(integrated_sql)\n",
    "integrated_df.createOrReplaceTempView(\"columns_with_constraints\")\n",
    "\n",
    "column_count = integrated_df.count()\n",
    "print(f\"取得カラム数: {column_count}\")\n",
    "\n",
    "if column_count > 0:\n",
    "    display(integrated_df.limit(5))\n",
    "    display(integrated_df.filter(F.col('data_type') == 'DECIMAL').limit(3))\n",
    "else:\n",
    "    print(\"対象カラムはありません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5340119d-3f5b-45f8-8ac8-a05aef4e9596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. テーブル詳細(DESCRIBE DETAIL)情報を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "968b547f-d002-44d9-8a2e-c305e692d1de",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DESCRIBE DETAIL 情報取得用処理"
    }
   },
   "outputs": [],
   "source": [
    "def describe_detail(catalog: str, schema: str, table: str) -> dict:\n",
    "    \"\"\"\n",
    "    DESCRIBE DETAIL catalog.schema.table を1回実行して dict 化\n",
    "    properties (MAP) は JSON 化もしておく\n",
    "    \"\"\"\n",
    "    full_quoted = fqname(catalog, schema, table) \n",
    "    df = spark.sql(f\"DESCRIBE DETAIL {full_quoted}\")\n",
    "    row = df.first()\n",
    "    if row is None:\n",
    "        return {\n",
    "            \"catalog_name\": catalog,\n",
    "            \"schema_name\": schema,\n",
    "            \"table_name\": table,\n",
    "            \"full_table_name\": f\"{catalog}.{schema}.{table}\",\n",
    "            \"error\": \"DESCRIBE DETAIL returned no rows\"\n",
    "        }\n",
    "    d = row.asDict(recursive=True)\n",
    "\n",
    "    # properties は MapType(string,string)（ない場合もある）。JSON化しておく\n",
    "    props = d.get(\"properties\")\n",
    "    if isinstance(props, dict):\n",
    "        d[\"properties_json\"] = json.dumps(props, ensure_ascii=False, separators=(',', ':'))\n",
    "    else:\n",
    "        d[\"properties_json\"] = None\n",
    "\n",
    "    # フィールドを付与\n",
    "    d.update({\n",
    "        \"catalog_name\": catalog,\n",
    "        \"schema_name\": schema,\n",
    "        \"table_name\": table,\n",
    "        \"full_table_name\": f\"{catalog}.{schema}.{table}\"\n",
    "    })\n",
    "    return d\n",
    "\n",
    "def extract_clustering_info(table_details: Dict[str, Any]) -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    DESCRIBE DETAIL の dict を前提に、クラスタリング方式と列を返す。\n",
    "    \"\"\"\n",
    "    strategy = 'NONE'\n",
    "    cols: List[str] = []\n",
    "\n",
    "    features = table_details.get('tableFeatures', []) or []\n",
    "    features_u = [str(f).upper() for f in features]\n",
    "\n",
    "    props: Dict[str, Any] = table_details.get('properties', {}) or {}\n",
    "\n",
    "    # Liquid clustering\n",
    "    if 'LIQUID_CLUSTERING' in features_u:\n",
    "        strategy = 'LIQUID'\n",
    "        for key in ('delta.clusterBy', 'delta.liquidClustering.columns', 'delta.clusteredColumns'):\n",
    "            v = props.get(key)\n",
    "            if isinstance(v, str) and v.strip():\n",
    "                cols = [c.strip() for c in v.split(',')]\n",
    "                break\n",
    "\n",
    "    # Z-Order\n",
    "    if strategy == 'NONE':\n",
    "        z_keys = [k for k in props.keys()\n",
    "                  if k.lower().startswith('delta.zorder') or k.lower().endswith('zorderby')]\n",
    "        if z_keys:\n",
    "            strategy = 'ZORDER'\n",
    "            for zk in z_keys:\n",
    "                v = props.get(zk)\n",
    "                if isinstance(v, str) and v.strip():\n",
    "                    cols = [c.strip() for c in v.split(',')]\n",
    "                    break\n",
    "\n",
    "    return strategy, cols\n",
    "\n",
    "def extract_partition_info(table_details: Dict[str, Any]) -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    DESCRIBE DETAIL の partitionColumns（array<string>）をそのまま利用。\n",
    "    \"\"\"\n",
    "    cols = table_details.get('partitionColumns', []) or []\n",
    "    strategy = 'NONE' if not cols else 'BY_COLUMNS'\n",
    "    return strategy, [str(c) for c in cols]\n",
    "\n",
    "def _to_bool(s: Any) -> bool:\n",
    "    return str(s).strip().lower() in ('true', '1', 'yes')\n",
    "\n",
    "def get_delta_properties(table_details: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    DESCRIBE DETAIL の properties/map と tableFeatures を用いて主な設定を抽出。\n",
    "    \"\"\"\n",
    "    props: Dict[str, Any] = table_details.get('properties', {}) or {}\n",
    "    features = table_details.get('tableFeatures', []) or []\n",
    "    features_u = [str(f).upper() for f in features]\n",
    "\n",
    "    out = {\n",
    "        'auto_optimize_write':   _to_bool(props.get('delta.autoOptimize.optimizeWrite', 'false')),\n",
    "        'auto_optimize_compact': _to_bool(props.get('delta.autoOptimize.autoCompact', 'false')),\n",
    "        'cdf_enabled':           _to_bool(props.get('delta.enableChangeDataFeed', 'false')) or\n",
    "                                 ('CHANGE_DATA_FEED' in features_u),\n",
    "        'stats_column_limit':    int(props.get('delta.dataSkippingNumIndexedCols', 32) or 32),\n",
    "        'stats_custom_columns':  None,\n",
    "        'vacuum_retention_hours': 168,   # default\n",
    "        'time_travel_retention_days': 30 # default\n",
    "    }\n",
    "\n",
    "    # dataSkipping のカスタム列\n",
    "    scols = props.get('delta.dataSkippingStatsColumns')\n",
    "    if isinstance(scols, str) and scols.strip():\n",
    "        out['stats_custom_columns'] = [c.strip() for c in scols.split(',')]\n",
    "\n",
    "    # vacuum の保持（例: \"interval 168 hours\" / \"168 hours\"）\n",
    "    vstr = props.get('delta.deletedFileRetentionDuration')\n",
    "    if isinstance(vstr, str) and vstr:\n",
    "        m = re.search(r'(\\d+)\\s*hour', vstr, re.I)\n",
    "        if m:\n",
    "            out['vacuum_retention_hours'] = int(m.group(1))\n",
    "\n",
    "    # time travel の保持（例: \"interval 30 days\" / \"30 days\"）\n",
    "    lstr = props.get('delta.logRetentionDuration')\n",
    "    if isinstance(lstr, str) and lstr:\n",
    "        m = re.search(r'(\\d+)\\s*day', lstr, re.I)\n",
    "        if m:\n",
    "            out['time_travel_retention_days'] = int(m.group(1))\n",
    "\n",
    "    return out\n",
    "\n",
    "def list_target_tables():\n",
    "    \"\"\"base_tables からクオート付きで返す\"\"\"\n",
    "    rows = spark.sql(\"\"\"\n",
    "        SELECT DISTINCT table_catalog, table_schema, table_name\n",
    "        FROM base_tables\n",
    "        ORDER BY table_catalog, table_schema, table_name\n",
    "    \"\"\").collect()\n",
    "\n",
    "    out = []\n",
    "    for r in rows:\n",
    "        out.append(Row(\n",
    "            table_catalog=r.table_catalog,\n",
    "            table_schema=r.table_schema,\n",
    "            table_name=r.table_name,\n",
    "            full_table_name_quoted=fqname(r.table_catalog, r.table_schema, r.table_name),\n",
    "        ))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ff64602-652d-47fe-8b96-3b4707908c20",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DESCRIBE DETAIL の実行処理"
    }
   },
   "outputs": [],
   "source": [
    "def collect_details(table_rows, max_workers=None, limit=None):\n",
    "    if max_workers is None:\n",
    "        max_workers = CONFIG.get(\"max_parallel_workers\", 4)\n",
    "    \n",
    "    it = table_rows if limit is None else table_rows[:limit]\n",
    "    print(f\"実行開始: {len(it)}テーブル, {max_workers}並列\")\n",
    "    log_each = (len(it) <= 30)\n",
    "    \n",
    "    def process_table(r):\n",
    "        try:\n",
    "            return describe_detail(r.table_catalog, r.table_schema, r.table_name)\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"catalog_name\": r.table_catalog,\n",
    "                \"schema_name\": r.table_schema,\n",
    "                \"table_name\": r.table_name,\n",
    "                \"full_table_name\": f\"{r.table_catalog}.{r.table_schema}.{r.table_name}\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_table = {executor.submit(process_table, r): r for r in it}\n",
    "        \n",
    "        for future in as_completed(future_to_table):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "            \n",
    "            if log_each:\n",
    "                status = \"OK\" if 'error' not in result else \"ERR\"\n",
    "                progress = f\"({len(results)}/{len(it)})\"\n",
    "                print(f\"{status} {progress}: {result['full_table_name']}\")\n",
    "    \n",
    "    def sort_key(result):\n",
    "        return (result.get('catalog_name', ''), \n",
    "                result.get('schema_name', ''), \n",
    "                result.get('table_name', ''))\n",
    "    \n",
    "    results.sort(key=sort_key)\n",
    "    \n",
    "    success_count = len([r for r in results if 'error' not in r])\n",
    "    print(f\"実行完了: 成功 {success_count}/{len(results)}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d20645f9-cf72-4d5e-9583-df3b9996e7f8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "対象テーブル一覧と詳細収集の実行"
    }
   },
   "outputs": [],
   "source": [
    "# 対象のリストの確認\n",
    "tables = list_target_tables()\n",
    "\n",
    "table_details = collect_details(tables, limit=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddc202c2-a118-4c26-a12d-f93c03984643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. テーブル詳細情報を取得の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffdf963a-b27c-4eb3-91d1-0f31c3dd8e84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _human_bytes(n: Any) -> Optional[str]:\n",
    "    \"\"\"バイト数に単位をつけて返す\"\"\"\n",
    "    if n is None:\n",
    "        return None\n",
    "    try:\n",
    "        n = int(n)\n",
    "    except Exception:\n",
    "        return None\n",
    "    units = [\"B\",\"KB\",\"MB\",\"GB\",\"TB\",\"PB\"]\n",
    "    i = 0\n",
    "    x = float(n)\n",
    "    while x >= 1024 and i < len(units)-1:\n",
    "        x /= 1024.0\n",
    "        i += 1\n",
    "    return f\"{x:.2f} {units[i]}\"\n",
    "\n",
    "def build_table_ddl_info(table_details: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"TABLE_DDL_INFO構築のメイン処理\"\"\"\n",
    "    idx: Dict[Tuple[str, str, str], Dict[str, Any]] = {}\n",
    "    for d in table_details:\n",
    "        k = (d.get('catalog_name'), d.get('schema_name'), d.get('table_name'))\n",
    "        if all(k):\n",
    "            idx[k] = d\n",
    "\n",
    "    base_rows = spark.sql(\"\"\"\n",
    "        SELECT table_catalog, table_schema, table_name, table_type, table_owner, created, last_altered\n",
    "        FROM base_tables\n",
    "        ORDER BY table_catalog, table_schema, table_name\n",
    "    \"\"\").collect()\n",
    "\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for r in base_rows:\n",
    "        key = (r.table_catalog, r.table_schema, r.table_name)\n",
    "        detail = idx.get(key, {})\n",
    "        \n",
    "        cluster_strategy, cluster_cols = extract_clustering_info(detail)\n",
    "        partition_strategy, partition_cols = extract_partition_info(detail)\n",
    "        delta_props = get_delta_properties(detail) if detail else {\n",
    "            'auto_optimize_write': False, 'auto_optimize_compact': False, 'cdf_enabled': False,\n",
    "            'stats_column_limit': 32, 'stats_custom_columns': None,\n",
    "            'vacuum_retention_hours': 168, 'time_travel_retention_days': 30,\n",
    "        }\n",
    "        \n",
    "        rec = {\n",
    "            'catalog_name': r.table_catalog, 'schema_name': r.table_schema, 'table_name': r.table_name, 'table_type': r.table_type,\n",
    "            'table_owner': r.table_owner,\n",
    "            'storage_format': (None if r.table_type == 'VIEW' else detail.get('format')), 'storage_location': (None if r.table_type == 'VIEW' else detail.get('location')),\n",
    "            'external_location': (detail.get('location') if r.table_type == 'EXTERNAL' else None),\n",
    "            'partition_strategy': partition_strategy, 'partition_columns': partition_cols,\n",
    "            'clustering_strategy': cluster_strategy, 'clustering_columns': cluster_cols,\n",
    "            'auto_optimize_write': delta_props['auto_optimize_write'], 'auto_optimize_compact': delta_props['auto_optimize_compact'],\n",
    "            'vacuum_retention_hours': delta_props['vacuum_retention_hours'], 'stats_column_limit': delta_props['stats_column_limit'],\n",
    "            'stats_custom_columns': delta_props['stats_custom_columns'], 'cdf_enabled': delta_props['cdf_enabled'],\n",
    "            'time_travel_retention_days': delta_props['time_travel_retention_days'],\n",
    "            'num_files': detail.get('numFiles'), 'size_in_bytes': detail.get('sizeInBytes'), 'size_pretty': _human_bytes(detail.get('sizeInBytes')),\n",
    "            'table_features': [str(f) for f in (detail.get('tableFeatures') or [])], 'table_id': detail.get('id'),\n",
    "            'created_at': detail.get('createdAt', r.created), 'last_altered': detail.get('lastModified', r.last_altered),\n",
    "            'extracted_at': datetime.now(timezone.utc).isoformat(), 'extraction_method': 'detail_python',\n",
    "        }\n",
    "        \n",
    "        if 'error' in detail:\n",
    "            rec['detail_error'] = detail['error']\n",
    "            \n",
    "        out.append(rec)\n",
    "\n",
    "    return out\n",
    "\n",
    "def build_column_ddl_info(table_details: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"COLUMN_DDL_INFO構築のメイン処理（統合版）\"\"\"\n",
    "    # パーティション・クラスタ列のマップ作成\n",
    "    part_cols_map: Dict[Tuple[str, str, str], Set[str]] = {}\n",
    "    clus_cols_map: Dict[Tuple[str, str, str], Set[str]] = {}\n",
    "\n",
    "    for d in table_details:\n",
    "        key = (d.get('catalog_name'), d.get('schema_name'), d.get('table_name'))\n",
    "        if not all(key):\n",
    "            continue\n",
    "        _, part_cols = extract_partition_info(d)\n",
    "        part_cols_map[key] = set([c.lower() for c in part_cols])\n",
    "        _, clus_cols = extract_clustering_info(d)\n",
    "        clus_cols_map[key] = set([c.lower() for c in clus_cols])\n",
    "\n",
    "    # 統合クエリから列+制約情報を取得\n",
    "    rows = spark.sql(\"\"\"\n",
    "        SELECT * FROM columns_with_constraints\n",
    "        ORDER BY table_catalog, table_schema, table_name, ordinal_position\n",
    "    \"\"\").collect()\n",
    "\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    now_utc = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    for r in rows:\n",
    "        key = (r.table_catalog, r.table_schema, r.table_name)\n",
    "        pset = part_cols_map.get(key, set())\n",
    "        cset = clus_cols_map.get(key, set())\n",
    "        \n",
    "        col_name_lower = r.column_name.lower()\n",
    "\n",
    "        rec = {\n",
    "            \"catalog_name\": r.table_catalog, \"schema_name\": r.table_schema, \"table_name\": r.table_name, \"column_name\": r.column_name,\n",
    "            \"ordinal_position\": int(r.ordinal_position), \"data_type\": r.data_type, \"numeric_precision\": r.numeric_precision, \"numeric_scale\": r.numeric_scale,\n",
    "            \"is_nullable\": (str(r.is_nullable).upper() == \"YES\"), \"default_value\": r.column_default,\n",
    "            \"column_comment\": getattr(r, \"comment\", None) or getattr(r, \"column_comment\", None),\n",
    "            \"is_partition_column\": col_name_lower in pset, \"is_clustering_column\": col_name_lower in cset,\n",
    "            \"is_primary_key\": bool(r.is_pk), \"foreign_key_reference\": None if not r.is_fk else \"FK参照あり\",\n",
    "            \"extracted_at\": now_utc, \"extraction_method\": \"detail_python\",\n",
    "        }\n",
    "        \n",
    "        out.append(rec)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b78d943-9ba2-4e46-a0de-8101a051ec87",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "TABLE_DDL_INFO の構築実行"
    }
   },
   "outputs": [],
   "source": [
    "# メタデータ構築を実行（テーブル情報はPython、カラム情報はSparkで後段生成）\n",
    "table_ddl_data = build_table_ddl_info(table_details)\n",
    "print(f\"TABLE_DDL_INFO構築完了: {len(table_ddl_data)}件\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93fa7f84-687b-447d-b361-eed8a9b16045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. カラム情報の取得実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a401b4d2-f099-46c2-9f9d-ea971b5be1ee",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "COLUMN_DDL_INFO DataFrame の作成"
    }
   },
   "outputs": [],
   "source": [
    "# COLUMN_DDL_INFO DataFrame作成（Sparkネイティブ。collectを回避）\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# columns_with_constraints は既存のSparkビュー\n",
    "cols_df = spark.table(\"columns_with_constraints\")\n",
    "\n",
    "# パーティション列・クラスタ列のインジケータを table_details から作成し、結合で付与\n",
    "part_pairs = []\n",
    "clus_pairs = []\n",
    "for d in table_details:\n",
    "    key = (d.get('catalog_name'), d.get('schema_name'), d.get('table_name'))\n",
    "    if not all(key):\n",
    "        continue\n",
    "    _, pcols = extract_partition_info(d)\n",
    "    for c in pcols:\n",
    "        part_pairs.append((*key, c.lower()))\n",
    "    _, zcols = extract_clustering_info(d)\n",
    "    for c in zcols:\n",
    "        clus_pairs.append((*key, c.lower()))\n",
    "\n",
    "part_df = spark.createDataFrame(part_pairs, ['table_catalog','table_schema','table_name','col_lower']).dropDuplicates() if part_pairs else None\n",
    "clus_df = spark.createDataFrame(clus_pairs, ['table_catalog','table_schema','table_name','col_lower']).dropDuplicates() if clus_pairs else None\n",
    "\n",
    "df = cols_df.withColumn('col_lower', F.lower('column_name'))\n",
    "if part_df is not None:\n",
    "    df = df.join(part_df.withColumn('p_hit', F.lit(True)), ['table_catalog','table_schema','table_name','col_lower'], 'left')\n",
    "else:\n",
    "    df = df.withColumn('p_hit', F.lit(None))\n",
    "if clus_df is not None:\n",
    "    df = df.join(clus_df.withColumn('z_hit', F.lit(True)), ['table_catalog','table_schema','table_name','col_lower'], 'left')\n",
    "else:\n",
    "    df = df.withColumn('z_hit', F.lit(None))\n",
    "\n",
    "now_utc = datetime.now(timezone.utc).isoformat()\n",
    "column_ddl_df = (df\n",
    "    .withColumn('is_partition_column', F.when(F.col('p_hit').isNotNull(), F.lit(True)).otherwise(F.lit(False)))\n",
    "    .withColumn('is_clustering_column', F.when(F.col('z_hit').isNotNull(), F.lit(True)).otherwise(F.lit(False)))\n",
    "    .withColumn('is_primary_key', F.col('is_pk').cast('boolean'))\n",
    "    .withColumn('foreign_key_reference', F.when(F.col('is_fk') == 1, F.lit('FK参照あり')).otherwise(F.lit(None)))\n",
    "    .withColumn('extracted_at', F.lit(now_utc))\n",
    "    .withColumn('extraction_method', F.lit('detail_python'))\n",
    ")\n",
    "\n",
    "select_cols = [\n",
    "    'table_catalog','table_schema','table_name','column_name','ordinal_position','data_type','numeric_precision','numeric_scale',\n",
    "    'is_nullable','column_default','comment','column_comment','is_partition_column','is_clustering_column','is_primary_key',\n",
    "    'foreign_key_reference','extracted_at','extraction_method'\n",
    "]\n",
    "# 一部の列名は存在しない可能性があるため、存在チェック\n",
    "existing = [c for c in select_cols if c in column_ddl_df.columns]\n",
    "column_ddl_df = column_ddl_df.select(*existing)\n",
    "column_ddl_df = (column_ddl_df\n",
    "    .withColumnRenamed('table_catalog','catalog_name')\n",
    "    .withColumnRenamed('table_schema','schema_name')\n",
    " )\n",
    "\n",
    "column_ddl_df.createOrReplaceTempView('column_ddl_info')\n",
    "print(f\"COLUMN_DDL_INFO DataFrame作成完了: {column_ddl_df.count()} レコード\")\n",
    "display(column_ddl_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af5abab2-67e8-4ff5-b75f-1d36dc92830f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "TABLE_DDL_INFO DataFrame の作成"
    }
   },
   "outputs": [],
   "source": [
    "# TABLE_DDL_INFO DataFrame作成\n",
    "if table_ddl_data:\n",
    "    table_schema = StructType([\n",
    "        StructField(\"catalog_name\", StringType(), False),\n",
    "        StructField(\"schema_name\",  StringType(), False),\n",
    "        StructField(\"table_name\",   StringType(), False),\n",
    "        StructField(\"table_type\",   StringType(), False),\n",
    "        StructField(\"table_owner\",  StringType(), True),\n",
    "        StructField(\"storage_format\",   StringType(), True),\n",
    "        StructField(\"storage_location\", StringType(), True),\n",
    "        StructField(\"external_location\", StringType(), True),\n",
    "        StructField(\"partition_strategy\", StringType(), False),\n",
    "        StructField(\"partition_columns\",  ArrayType(StringType()), True),\n",
    "        StructField(\"clustering_strategy\", StringType(), False),\n",
    "        StructField(\"clustering_columns\",  ArrayType(StringType()), True),\n",
    "        StructField(\"auto_optimize_write\",   BooleanType(), False),\n",
    "        StructField(\"auto_optimize_compact\", BooleanType(), False),\n",
    "        StructField(\"vacuum_retention_hours\", IntegerType(), True),\n",
    "        StructField(\"stats_column_limit\",     IntegerType(), True),\n",
    "        StructField(\"stats_custom_columns\",   ArrayType(StringType()), True),\n",
    "        StructField(\"cdf_enabled\",                 BooleanType(), False),\n",
    "        StructField(\"time_travel_retention_days\",  IntegerType(), True),\n",
    "        StructField(\"num_files\",     LongType(), True),\n",
    "        StructField(\"size_in_bytes\", LongType(), True),\n",
    "        StructField(\"size_pretty\",   StringType(), True),\n",
    "        StructField(\"table_features\", ArrayType(StringType()), True),\n",
    "        StructField(\"table_id\",     StringType(), True),\n",
    "        StructField(\"created_at\",   StringType(), True),\n",
    "        StructField(\"last_altered\", StringType(), True),\n",
    "        StructField(\"extracted_at\", StringType(), False),\n",
    "        StructField(\"extraction_method\", StringType(), True),\n",
    "        StructField(\"detail_error\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "    table_rows = []\n",
    "    for r in table_ddl_data:\n",
    "        row = (\n",
    "            r['catalog_name'], r['schema_name'], r['table_name'], r['table_type'], r.get('table_owner'),\n",
    "            r.get('storage_format'), r.get('storage_location'), r.get('external_location'),\n",
    "            r['partition_strategy'], r.get('partition_columns') or [],\n",
    "            r['clustering_strategy'], r.get('clustering_columns') or [],\n",
    "            bool(r.get('auto_optimize_write', False)), bool(r.get('auto_optimize_compact', False)),\n",
    "            r.get('vacuum_retention_hours'), r.get('stats_column_limit'), r.get('stats_custom_columns') or [],\n",
    "            bool(r.get('cdf_enabled', False)), r.get('time_travel_retention_days'),\n",
    "            r.get('num_files'), r.get('size_in_bytes'), r.get('size_pretty'), r.get('table_features') or [],\n",
    "            r.get('table_id'), r.get('created_at'), r.get('last_altered'),\n",
    "            r['extracted_at'], r.get('extraction_method'), r.get('detail_error')\n",
    "        )\n",
    "        table_rows.append(row)\n",
    "\n",
    "    table_ddl_df = spark.createDataFrame(table_rows, table_schema)\n",
    "    table_ddl_df.createOrReplaceTempView(\"table_ddl_info\")\n",
    "    print(f\"TABLE_DDL_INFO DataFrame作成完了: {table_ddl_df.count()} レコード\")\n",
    "    display(table_ddl_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "435ba2b7-d6fc-4083-b64b-dd51642f22bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. メタデータの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "041a97fe-6d3b-40b6-a6c5-2a410e624cde",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "メタデータの保存"
    }
   },
   "outputs": [],
   "source": [
    "# 統合設定を使用したデータ保存とエクスポート\n",
    "meta_catalog = CONFIG[\"output_catalog\"]\n",
    "meta_schema  = CONFIG.get(\"output_schema\", CONFIG[\"target_catalog\"])\n",
    "\n",
    "# メタデータ保存先がない場合、作成\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS `{meta_catalog}`\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{meta_catalog}`.`{meta_schema}`\")\n",
    "\n",
    "table_df  = spark.sql(\"SELECT * FROM table_ddl_info\")\n",
    "column_df = spark.sql(\"SELECT * FROM column_ddl_info\")\n",
    "\n",
    "# スナップショット時刻を付与（任意）\n",
    "snap_ts = datetime.now(timezone.utc)\n",
    "table_df  = table_df.withColumn(\"snapshot_at\", F.lit(snap_ts))\n",
    "column_df = column_df.withColumn(\"snapshot_at\", F.lit(snap_ts))\n",
    "\n",
    "# 最新版テーブルとして上書き保存（Managed Delta）\n",
    "(table_df\n",
    " .write.mode(\"overwrite\")\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .saveAsTable(f\"`{meta_catalog}`.`{meta_schema}`.`{meta_schema}_table_ddl_info`\"))\n",
    "\n",
    "(column_df\n",
    " .write.mode(\"overwrite\")\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .saveAsTable(f\"`{meta_catalog}`.`{meta_schema}`.`{meta_schema}_column_ddl_info`\"))\n",
    "\n",
    "# 保持期間設定\n",
    "retention_days = CONFIG[\"retention_days\"]\n",
    "\n",
    "spark.sql(f\"\"\"  \n",
    "        ALTER TABLE `{meta_catalog}`.`{meta_schema}`.`{meta_schema}_table_ddl_info`  SET TBLPROPERTIES\n",
    "        ('delta.logRetentionDuration'='interval {retention_days} days',\n",
    "        'delta.deletedFileRetentionDuration'='interval {retention_days} days')\n",
    "        \"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "        ALTER TABLE `{meta_catalog}`.`{meta_schema}`.`{meta_schema}_column_ddl_info` SET TBLPROPERTIES\n",
    "        ('delta.logRetentionDuration'='interval {retention_days} days',\n",
    "        'delta.deletedFileRetentionDuration'='interval {retention_days} days')\n",
    "        \"\"\")\n",
    "\n",
    "print(f\"メタデータ保存完了: {meta_catalog}.{meta_schema}.*_ddl_info\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "unity_catalog_metadata_extractor",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
