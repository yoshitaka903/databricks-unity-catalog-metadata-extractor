{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "407f0980-d121-4c53-843a-e69addc08db9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Unity Catalog メタデータ抽出ノートブック\n",
    "\n",
    "このノートブックはUnity Catalogから既存テーブルのメタデータを抽出し、TABLE_DDL_INFO と COLUMN_DDL_INFO の形式で整理します。\n",
    "\n",
    "## 取得対象\n",
    "- ◎: 完全自動取得可能項目\n",
    "- ○: 条件付き自動取得可能項目  \n",
    "- △: 推測可能項目\n",
    "- ×: 手動管理必須項目（空値で初期化）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2b9ebbb-eab6-4e78-a90a-1a8f50bdd501",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. 初期設定・パラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fface55-5a3e-4b7f-9f49-ee5a5b944f34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ライブラリ\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from typing import Dict, Any, List, Tuple, Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "526c9750-f40e-4132-91dc-6c7d19cd87ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 対象のカタログを指定\n",
    "catalog = \"samples\"\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "\n",
    "# カタログ配下のスキーマ一覧を取得\n",
    "schemas_sql = f\"\"\"\n",
    "SELECT \n",
    "    catalog_name, \n",
    "    schema_name, \n",
    "    schema_owner, \n",
    "    created, \n",
    "    last_altered \n",
    "FROM {catalog}.information_schema.schemata\n",
    "WHERE 1=1\n",
    "  AND schema_name <> 'information_schema'\n",
    "ORDER BY catalog_name, schema_name\n",
    "\"\"\"\n",
    "\n",
    "schemas_df = spark.sql(schemas_sql)\n",
    "display(schemas_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30c5a221-0c1b-4219-b771-9ef8c4cf0365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# カタログ配下のスキーマ一覧を取得・確認\n",
    "schema_list = [row[\"schema_name\"] for row in schemas_df.select(\"schema_name\").collect()]\n",
    "schema_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15ba1cdc-87cf-4934-bc28-19cb255b03d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# メタデータ取得対象のスキーマを絞りたい場合、schema_listを更新する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c48be364-84bc-42c4-8df2-3caf05683eae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 統合設定（重複削除・一元管理）\n",
    "CONFIG = {\n",
    "    \"target_catalog\": catalog,\n",
    "    \"include_schemas\": schema_list,\n",
    "    \"output_catalog\": \"ops\",\n",
    "    \"output_path\": \"\",\n",
    "    \"exclude_patterns\": ['^__', '^event_log_'],\n",
    "    \"table_types\": ['MANAGED', 'EXTERNAL', 'VIEW'],\n",
    "    \"retention_days\": 180,\n",
    "    \"max_parallel_workers\": 4,\n",
    "    \"describe_detail_timeout\": 30\n",
    "}\n",
    "\n",
    "print(f\"対象カタログ: {CONFIG['target_catalog']}\")\n",
    "print(f\"対象スキーマ: {CONFIG['include_schemas']}\")\n",
    "print(f\"実行時刻: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6062c927-5ca8-4d1b-a2cc-0f2ae5f1743d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. 共通処理・ユーティリティ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee10d663-5dd0-4180-af10-0e83ea6df15b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_filter_conditions(config):\n",
    "    \"\"\"共通フィルタ条件を生成（重複削除）\"\"\"\n",
    "    schema_filter = \"', '\".join(config[\"include_schemas\"])\n",
    "    exclude_where = \" AND \".join([f\"table_name NOT RLIKE '{p}'\" for p in config[\"exclude_patterns\"]])\n",
    "    table_types = \"', '\".join(config[\"table_types\"])\n",
    "    return schema_filter, exclude_where, table_types\n",
    "\n",
    "def q(identifier: str) -> str:\n",
    "    \"\"\"Spark SQL用にバッククォートでエスケープ\"\"\"\n",
    "    return f\"`{identifier.replace('`', '``')}`\"\n",
    "\n",
    "def fqname(catalog: str, schema: str, table: str) -> str:\n",
    "    \"\"\"完全修飾名を生成\"\"\"\n",
    "    return f\"{q(catalog)}.{q(schema)}.{q(table)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e4f15f4-cbfc-4ed8-bd95-bdc2c4e5fa98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. テーブル基本情報取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c226d321-b2da-4157-b23f-1aacaf4e5e60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 共通フィルタ条件を使用\n",
    "schema_filter, exclude_where, table_types = build_filter_conditions(CONFIG)\n",
    "\n",
    "tables_sql = f\"\"\"\n",
    "SELECT \n",
    "    table_catalog,\n",
    "    table_schema,\n",
    "    table_name,\n",
    "    table_type,\n",
    "    created,\n",
    "    last_altered\n",
    "FROM {CONFIG['target_catalog']}.information_schema.tables \n",
    "WHERE 1=1\n",
    "  AND table_schema IN ('{schema_filter}')\n",
    "  AND table_type IN ('{table_types}')\n",
    "  AND {exclude_where}\n",
    "ORDER BY table_catalog, table_schema, table_name\n",
    "\"\"\"\n",
    "\n",
    "tables_df = spark.sql(tables_sql)\n",
    "tables_df.createOrReplaceTempView(\"base_tables\")\n",
    "\n",
    "table_count = tables_df.count()\n",
    "print(f\"取得テーブル数: {table_count}\")\n",
    "\n",
    "if table_count > 0:\n",
    "    display(tables_df.limit(5))\n",
    "else:\n",
    "    print(\"対象テーブルはありません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1efbb564-b434-4112-8ae1-5d5aef33e0b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. カラム基本情報の取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "723c38ca-e9d9-460f-ae0d-b77dbe6912a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 同じフィルタ条件を再利用\n",
    "columns_sql = f\"\"\"\n",
    "SELECT \n",
    "    table_catalog,\n",
    "    table_schema,\n",
    "    table_name,\n",
    "    column_name,\n",
    "    ordinal_position + 1 as ordinal_position,\n",
    "    data_type,\n",
    "    is_nullable,\n",
    "    column_default,\n",
    "    numeric_precision,\n",
    "    numeric_scale\n",
    "FROM {CONFIG['target_catalog']}.information_schema.columns \n",
    "WHERE 1=1 \n",
    "  AND table_schema IN ('{schema_filter}')\n",
    "  AND {exclude_where}\n",
    "ORDER BY table_catalog, table_schema, table_name, ordinal_position\n",
    "\"\"\"\n",
    "\n",
    "columns_df = spark.sql(columns_sql)\n",
    "columns_df.createOrReplaceTempView(\"base_columns\")\n",
    "\n",
    "column_count = columns_df.count()\n",
    "print(f\"取得カラム数: {column_count}\")\n",
    "\n",
    "if column_count > 0:\n",
    "    display(columns_df.limit(5))\n",
    "    display(columns_df.filter(F.column('data_type')== 'DECIMAL').limit(5))\n",
    "else:\n",
    "    print(\"対象カラムはありません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "090a6f47-fd4a-420a-87d1-5333e6303e44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "## 4.5. PK/FK制約情報の取得"
  },
  {
   "cell_type": "code",
   "source": "# FOREIGN KEY制約情報を取得\nfk_constraints_sql = f\"\"\"\nSELECT \n    tc.constraint_catalog,\n    tc.constraint_schema,\n    tc.table_name,\n    tc.constraint_name,\n    tc.constraint_type,\n    kcu.column_name,\n    rc.referenced_table_catalog,\n    rc.referenced_table_schema, \n    rc.referenced_table_name,\n    rc.referenced_column_name\nFROM {CONFIG['target_catalog']}.information_schema.table_constraints tc\nJOIN {CONFIG['target_catalog']}.information_schema.key_column_usage kcu \n    ON tc.constraint_catalog = kcu.constraint_catalog \n    AND tc.constraint_schema = kcu.constraint_schema\n    AND tc.constraint_name = kcu.constraint_name\nJOIN {CONFIG['target_catalog']}.information_schema.referential_constraints rc\n    ON tc.constraint_catalog = rc.constraint_catalog\n    AND tc.constraint_schema = rc.constraint_schema  \n    AND tc.constraint_name = rc.constraint_name\nWHERE tc.constraint_type = 'FOREIGN KEY'\n  AND tc.constraint_schema IN ('{schema_filter}')\n  AND {exclude_where.replace('table_name', 'tc.table_name')}\nORDER BY tc.constraint_catalog, tc.constraint_schema, tc.table_name, kcu.ordinal_position\n\"\"\"\n\ntry:\n    fk_constraints_df = spark.sql(fk_constraints_sql)\n    fk_constraints_df.createOrReplaceTempView(\"fk_constraints\")\n    fk_count = fk_constraints_df.count()\n    print(f\"取得FOREIGN KEY制約数: {fk_count}\")\n    if fk_count > 0:\n        display(fk_constraints_df.limit(5))\nexcept Exception as e:\n    print(f\"FOREIGN KEY制約取得エラー (スキップ): {e}\")\n    fk_constraints_df = spark.createDataFrame([], StructType([\n        StructField(\"constraint_catalog\", StringType()),\n        StructField(\"constraint_schema\", StringType()),\n        StructField(\"table_name\", StringType()),\n        StructField(\"constraint_name\", StringType()),\n        StructField(\"constraint_type\", StringType()),\n        StructField(\"column_name\", StringType()),\n        StructField(\"referenced_table_catalog\", StringType()),\n        StructField(\"referenced_table_schema\", StringType()),\n        StructField(\"referenced_table_name\", StringType()),\n        StructField(\"referenced_column_name\", StringType())\n    ]))\n    fk_constraints_df.createOrReplaceTempView(\"fk_constraints\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# PRIMARY KEY制約情報を取得\npk_constraints_sql = f\"\"\"\nSELECT \n    constraint_catalog,\n    constraint_schema,\n    table_name,\n    constraint_name,\n    constraint_type,\n    column_names\nFROM {CONFIG['target_catalog']}.information_schema.table_constraints \nWHERE 1=1\n  AND constraint_type = 'PRIMARY KEY'\n  AND constraint_schema IN ('{schema_filter}')\n  AND {exclude_where.replace('table_name', 'table_name')}\nORDER BY constraint_catalog, constraint_schema, table_name\n\"\"\"\n\ntry:\n    pk_constraints_df = spark.sql(pk_constraints_sql)\n    pk_constraints_df.createOrReplaceTempView(\"pk_constraints\")\n    pk_count = pk_constraints_df.count()\n    print(f\"取得PRIMARY KEY制約数: {pk_count}\")\n    if pk_count > 0:\n        display(pk_constraints_df.limit(5))\nexcept Exception as e:\n    print(f\"PRIMARY KEY制約取得エラー (スキップ): {e}\")\n    pk_constraints_df = spark.createDataFrame([], StructType([\n        StructField(\"constraint_catalog\", StringType()),\n        StructField(\"constraint_schema\", StringType()),\n        StructField(\"table_name\", StringType()),\n        StructField(\"constraint_name\", StringType()),\n        StructField(\"constraint_type\", StringType()),\n        StructField(\"column_names\", ArrayType(StringType()))\n    ]))\n    pk_constraints_df.createOrReplaceTempView(\"pk_constraints\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b272fefb-d9ba-4c35-a092-3c6fc64972b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def describe_detail_one(catalog: str, schema: str, table: str) -> dict:\n",
    "    \"\"\"\n",
    "    DESCRIBE DETAIL catalog.schema.table を1回実行して dict 化\n",
    "    properties (MAP) は JSON 化もしておく\n",
    "    \"\"\"\n",
    "    full_quoted = fqname(catalog, schema, table) \n",
    "    df = spark.sql(f\"DESCRIBE DETAIL {full_quoted}\")\n",
    "    row = df.first()\n",
    "    if row is None:\n",
    "        return {\n",
    "            \"catalog_name\": catalog,\n",
    "            \"schema_name\": schema,\n",
    "            \"table_name\": table,\n",
    "            \"full_table_name\": f\"{catalog}.{schema}.{table}\",\n",
    "            \"error\": \"DESCRIBE DETAIL returned no rows\"\n",
    "        }\n",
    "    d = row.asDict(recursive=True)\n",
    "\n",
    "    # properties は MapType(string,string)（ない場合もある）。JSON化しておく\n",
    "    props = d.get(\"properties\")\n",
    "    if isinstance(props, dict):\n",
    "        d[\"properties_json\"] = json.dumps(props, ensure_ascii=False, separators=(',', ':'))\n",
    "    else:\n",
    "        d[\"properties_json\"] = None\n",
    "\n",
    "    # フィールドを付与\n",
    "    d.update({\n",
    "        \"catalog_name\": catalog,\n",
    "        \"schema_name\": schema,\n",
    "        \"table_name\": table,\n",
    "        \"full_table_name\": f\"{catalog}.{schema}.{table}\"\n",
    "    })\n",
    "    return d\n",
    "\n",
    "def extract_clustering_info(table_details: Dict[str, Any]) -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    DESCRIBE DETAIL の dict を前提に、クラスタリング方式と列を返す。\n",
    "    - Liquid: tableFeatures に 'LIQUID_CLUSTERING' が含まれる\n",
    "      列は properties の既知キー候補から取得（なければ空配列）\n",
    "    - Z-Order: properties に zorder 系キーがあれば検出（列は見つかったら取得）\n",
    "    \"\"\"\n",
    "    strategy = 'NONE'\n",
    "    cols: List[str] = []\n",
    "\n",
    "    features = table_details.get('tableFeatures', []) or []\n",
    "    features_u = [str(f).upper() for f in features]\n",
    "\n",
    "    props: Dict[str, Any] = table_details.get('properties', {}) or {}\n",
    "\n",
    "    # Liquid clustering\n",
    "    if 'LIQUID_CLUSTERING' in features_u:\n",
    "        strategy = 'LIQUID'\n",
    "        # 列候補キー\n",
    "        for key in ('delta.clusterBy', 'delta.liquidClustering.columns', 'delta.clusteredColumns'):\n",
    "            v = props.get(key)\n",
    "            if isinstance(v, str) and v.strip():\n",
    "                cols = [c.strip() for c in v.split(',')]\n",
    "                break\n",
    "\n",
    "    # Z-Order（リキッドクラスタリング推奨。使ってたら検出）\n",
    "    if strategy == 'NONE':\n",
    "        z_keys = [k for k in props.keys()\n",
    "                  if k.lower().startswith('delta.zorder') or k.lower().endswith('zorderby')]\n",
    "        if z_keys:\n",
    "            strategy = 'ZORDER'\n",
    "            for zk in z_keys:\n",
    "                v = props.get(zk)\n",
    "                if isinstance(v, str) and v.strip():\n",
    "                    cols = [c.strip() for c in v.split(',')]\n",
    "                    break\n",
    "\n",
    "    return strategy, cols\n",
    "\n",
    "def extract_partition_info(table_details: Dict[str, Any]) -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    DESCRIBE DETAIL の partitionColumns（array<string>）をそのまま利用。\n",
    "    \"\"\"\n",
    "    cols = table_details.get('partitionColumns', []) or []\n",
    "    strategy = 'NONE' if not cols else 'BY_COLUMNS'\n",
    "    return strategy, [str(c) for c in cols]\n",
    "\n",
    "def _to_bool(s: Any) -> bool:\n",
    "    return str(s).strip().lower() in ('true', '1', 'yes')\n",
    "\n",
    "def get_delta_properties(table_details: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    DESCRIBE DETAIL の properties/map と tableFeatures を用いて主な設定を抽出。\n",
    "    - CDF: delta.enableChangeDataFeed or tableFeatures の CHANGE_DATA_FEED\n",
    "    - 自動最適化: delta.autoOptimize.optimizeWrite / delta.autoOptimize.autoCompact\n",
    "    - データスキッピング: 列数/カスタム列\n",
    "    - 保持: deletedFileRetentionDuration（hours）, logRetentionDuration（days）\n",
    "    \"\"\"\n",
    "    props: Dict[str, Any] = table_details.get('properties', {}) or {}\n",
    "    features = table_details.get('tableFeatures', []) or []\n",
    "    features_u = [str(f).upper() for f in features]\n",
    "\n",
    "    out = {\n",
    "        'auto_optimize_write':   _to_bool(props.get('delta.autoOptimize.optimizeWrite', 'false')),\n",
    "        'auto_optimize_compact': _to_bool(props.get('delta.autoOptimize.autoCompact', 'false')),\n",
    "        'cdf_enabled':           _to_bool(props.get('delta.enableChangeDataFeed', 'false')) or\n",
    "                                 ('CHANGE_DATA_FEED' in features_u),\n",
    "        'stats_column_limit':    int(props.get('delta.dataSkippingNumIndexedCols', 32) or 32),\n",
    "        'stats_custom_columns':  None,\n",
    "        'vacuum_retention_hours': 168,   # default\n",
    "        'time_travel_retention_days': 30 # default\n",
    "    }\n",
    "\n",
    "    # dataSkipping のカスタム列\n",
    "    scols = props.get('delta.dataSkippingStatsColumns')\n",
    "    if isinstance(scols, str) and scols.strip():\n",
    "        out['stats_custom_columns'] = [c.strip() for c in scols.split(',')]\n",
    "\n",
    "    # vacuum の保持（例: \"interval 168 hours\" / \"168 hours\"）\n",
    "    vstr = props.get('delta.deletedFileRetentionDuration')\n",
    "    if isinstance(vstr, str) and vstr:\n",
    "        m = re.search(r'(\\d+)\\s*hour', vstr, re.I)\n",
    "        if m:\n",
    "            out['vacuum_retention_hours'] = int(m.group(1))\n",
    "\n",
    "    # time travel の保持（例: \"interval 30 days\" / \"30 days\"）\n",
    "    lstr = props.get('delta.logRetentionDuration')\n",
    "    if isinstance(lstr, str) and lstr:\n",
    "        m = re.search(r'(\\d+)\\s*day', lstr, re.I)\n",
    "        if m:\n",
    "            out['time_travel_retention_days'] = int(m.group(1))\n",
    "\n",
    "    return out\n",
    "\n",
    "def list_target_tables():\n",
    "    \"\"\"base_tables からクオート付きで返す\"\"\"\n",
    "    rows = spark.sql(\"\"\"\n",
    "        SELECT DISTINCT table_catalog, table_schema, table_name\n",
    "        FROM base_tables\n",
    "        ORDER BY table_catalog, table_schema, table_name\n",
    "    \"\"\").collect()\n",
    "\n",
    "    out = []\n",
    "    for r in rows:\n",
    "        out.append(Row(\n",
    "            table_catalog=r.table_catalog,\n",
    "            table_schema=r.table_schema,\n",
    "            table_name=r.table_name,\n",
    "            full_table_name_quoted=fqname(r.table_catalog, r.table_schema, r.table_name),\n",
    "        ))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c3118b3-5321-4208-9704-be92f6ea7bd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def collect_details(table_rows, max_workers=None, limit=None):\n",
    "    \"\"\"並列版DESCRIBE DETAIL実行（推奨）\"\"\"\n",
    "    if max_workers is None:\n",
    "        max_workers = CONFIG.get(\"max_parallel_workers\", 4)\n",
    "    \n",
    "    it = table_rows if limit is None else table_rows[:limit]\n",
    "    print(f\"並列実行開始: {len(it)}テーブル, {max_workers}並列\")\n",
    "    \n",
    "    def process_table(r):\n",
    "        try:\n",
    "            return describe_detail_one(r.table_catalog, r.table_schema, r.table_name)\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"catalog_name\": r.table_catalog,\n",
    "                \"schema_name\": r.table_schema,\n",
    "                \"table_name\": r.table_name,\n",
    "                \"full_table_name\": f\"{r.table_catalog}.{r.table_schema}.{r.table_name}\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # 全テーブルを並列実行\n",
    "        future_to_table = {executor.submit(process_table, r): r for r in it}\n",
    "        \n",
    "        # 完了順に結果取得\n",
    "        for future in as_completed(future_to_table):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "            \n",
    "            # 進捗表示\n",
    "            status = \"OK\" if 'error' not in result else \"ERR\"\n",
    "            progress = f\"({len(results)}/{len(it)})\"\n",
    "            print(f\"{status} {progress}: {result['full_table_name']}\")\n",
    "    \n",
    "    # 元の順序で並び替え（table_catalog, table_schema, table_name順）\n",
    "    def sort_key(result):\n",
    "        return (result.get('catalog_name', ''), \n",
    "                result.get('schema_name', ''), \n",
    "                result.get('table_name', ''))\n",
    "    \n",
    "    results.sort(key=sort_key)\n",
    "    \n",
    "    success_count = len([r for r in results if 'error' not in r])\n",
    "    print(f\"並列実行完了: 成功 {success_count}/{len(results)}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f1d29ac-193d-4f13-ab09-f1fc3570dff4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 対象のリストの確認\n",
    "tables = list_target_tables()\n",
    "\n",
    "# 並列実行（推奨）- シリアル版より大幅に高速化\n",
    "table_details = collect_details(tables, limit=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "920d520f-87f5-4e32-890f-c05ea0fbdf25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. テーブルプロパティ解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "547e0fd5-a60d-4cb6-913b-a2ff01ebda6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _human_bytes(n: Any) -> str | None:\n",
    "    \"\"\"バイト数に単位をつけて返す\"\"\"\n",
    "    if n is None:\n",
    "        return None\n",
    "    try:\n",
    "        n = int(n)\n",
    "    except Exception:\n",
    "        return None\n",
    "    units = [\"B\",\"KB\",\"MB\",\"GB\",\"TB\",\"PB\"]\n",
    "    i = 0\n",
    "    x = float(n)\n",
    "    while x >= 1024 and i < len(units)-1:\n",
    "        x /= 1024.0\n",
    "        i += 1\n",
    "    return f\"{x:.2f} {units[i]}\"\n",
    "\n",
    "def create_detail_index(table_details: List[Dict[str, Any]]) -> Dict[Tuple[str, str, str], Dict[str, Any]]:\n",
    "    \"\"\"テーブル詳細情報のインデックス作成\"\"\"\n",
    "    idx: Dict[Tuple[str, str, str], Dict[str, Any]] = {}\n",
    "    for d in table_details:\n",
    "        k = (d.get('catalog_name'), d.get('schema_name'), d.get('table_name'))\n",
    "        if all(k):\n",
    "            idx[k] = d\n",
    "    return idx\n",
    "\n",
    "def build_single_table_record(base_row, detail: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"1つのテーブルのTABLE_DDL_INFOレコードを構築\"\"\"\n",
    "    cluster_strategy, cluster_cols = extract_clustering_info(detail)\n",
    "    partition_strategy, partition_cols = extract_partition_info(detail)\n",
    "    delta_props = get_delta_properties(detail) if detail else {\n",
    "        'auto_optimize_write': False,\n",
    "        'auto_optimize_compact': False,\n",
    "        'cdf_enabled': False,\n",
    "        'stats_column_limit': 32,\n",
    "        'stats_custom_columns': None,\n",
    "        'vacuum_retention_hours': 168,\n",
    "        'time_travel_retention_days': 30,\n",
    "    }\n",
    "\n",
    "    # 項目を追加\n",
    "    num_files     = detail.get('numFiles')\n",
    "    size_in_bytes = detail.get('sizeInBytes')\n",
    "    table_features = detail.get('tableFeatures') or []\n",
    "\n",
    "    rec = {\n",
    "        'catalog_name': base_row.table_catalog,\n",
    "        'schema_name':  base_row.table_schema,\n",
    "        'table_name':   base_row.table_name,\n",
    "        'table_type':   base_row.table_type,\n",
    "\n",
    "        'storage_format':   detail.get('format', 'DELTA'),\n",
    "        'storage_location': detail.get('location'),\n",
    "        'external_location': detail.get('location') if base_row.table_type == 'EXTERNAL' else None,\n",
    "\n",
    "        'partition_strategy':  partition_strategy,\n",
    "        'partition_columns':   partition_cols,\n",
    "        'partition_interval':  None,\n",
    "\n",
    "        'clustering_strategy': cluster_strategy,\n",
    "        'clustering_columns':  cluster_cols,\n",
    "\n",
    "        'auto_optimize_write':   delta_props['auto_optimize_write'],\n",
    "        'auto_optimize_compact': delta_props['auto_optimize_compact'],\n",
    "        'vacuum_retention_hours': delta_props['vacuum_retention_hours'],\n",
    "        'optimize_schedule':       None,\n",
    "        'stats_column_limit':      delta_props['stats_column_limit'],\n",
    "        'stats_custom_columns':    delta_props['stats_custom_columns'],\n",
    "        'predictive_optimization': None,\n",
    "        'stats_collection_strategy': None,\n",
    "\n",
    "        'cdf_enabled':                 delta_props['cdf_enabled'],\n",
    "        'time_travel_retention_days':  delta_props['time_travel_retention_days'],\n",
    "\n",
    "        'num_files':      num_files,\n",
    "        'size_in_bytes':  size_in_bytes,\n",
    "        'size_pretty':    _human_bytes(size_in_bytes),\n",
    "        'table_features': [str(f) for f in table_features],  # list[str]\n",
    "\n",
    "        'default_table_permissions': None,\n",
    "        'row_level_security':        None,\n",
    "        'data_quality_checks':       None,\n",
    "        'owner_email':       None,\n",
    "        'business_purpose':  None,\n",
    "        'update_frequency':  None,\n",
    "        'sla_requirements':  None,\n",
    "\n",
    "        'table_id':     detail.get('id'),\n",
    "        'created_at':   detail.get('createdAt', base_row.created),\n",
    "        'last_altered': detail.get('lastModified', base_row.last_altered),\n",
    "        'extracted_at': datetime.now(timezone.utc).isoformat(),\n",
    "        'extraction_method': 'detail_python',\n",
    "    }\n",
    "    \n",
    "    if 'error' in detail:\n",
    "        rec['detail_error'] = detail['error']\n",
    "        \n",
    "    return rec\n",
    "\n",
    "def build_table_ddl_info_detail(table_details: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"TABLE_DDL_INFO構築のメイン処理（分割後）\"\"\"\n",
    "    # 1. 索引化\n",
    "    idx = create_detail_index(table_details)\n",
    "\n",
    "    # 2. 基本テーブル情報取得\n",
    "    base_rows = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            table_catalog, table_schema, table_name, table_type,\n",
    "            created, last_altered\n",
    "        FROM base_tables\n",
    "        ORDER BY table_catalog, table_schema, table_name\n",
    "    \"\"\").collect()\n",
    "\n",
    "    # 3. 各テーブルのレコード構築\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for r in base_rows:\n",
    "        key = (r.table_catalog, r.table_schema, r.table_name)\n",
    "        detail = idx.get(key, {})\n",
    "        rec = build_single_table_record(r, detail)\n",
    "        out.append(rec)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92f1a319-787d-4217-8d73-6cb85a8b0d21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "def build_constraint_maps() -> Tuple[Dict[Tuple[str, str, str], Set[str]], Dict[Tuple[str, str, str], List[Dict[str, str]]]]:\n    \"\"\"制約情報をインデックス化（PK列 / FK参照情報）\"\"\"\n    pk_cols_map: Dict[Tuple[str, str, str], Set[str]] = {}\n    fk_refs_map: Dict[Tuple[str, str, str], List[Dict[str, str]]] = {}\n    \n    # PRIMARY KEY情報の処理\n    try:\n        pk_rows = spark.sql(\"SELECT * FROM pk_constraints\").collect()\n        for row in pk_rows:\n            table_key = (row.constraint_catalog, row.constraint_schema, row.table_name)\n            # column_names は配列として格納されている場合とカンマ区切り文字列の場合がある\n            col_names = row.column_names\n            if isinstance(col_names, list):\n                pk_cols_map[table_key] = set([c.lower() for c in col_names])\n            elif isinstance(col_names, str):\n                pk_cols_map[table_key] = set([c.strip().lower() for c in col_names.split(',')])\n            else:\n                pk_cols_map[table_key] = set()\n    except Exception as e:\n        print(f\"PRIMARY KEY制約処理エラー: {e}\")\n    \n    # FOREIGN KEY情報の処理\n    try:\n        fk_rows = spark.sql(\"SELECT * FROM fk_constraints\").collect()\n        for row in fk_rows:\n            table_key = (row.constraint_catalog, row.constraint_schema, row.table_name)\n            fk_info = {\n                'column_name': row.column_name,\n                'referenced_table': f\"{row.referenced_table_catalog}.{row.referenced_table_schema}.{row.referenced_table_name}\",\n                'referenced_column': row.referenced_column_name,\n                'constraint_name': row.constraint_name\n            }\n            if table_key not in fk_refs_map:\n                fk_refs_map[table_key] = []\n            fk_refs_map[table_key].append(fk_info)\n    except Exception as e:\n        print(f\"FOREIGN KEY制約処理エラー: {e}\")\n    \n    return pk_cols_map, fk_refs_map\n\ndef build_column_role_maps(table_details: List[Dict[str, Any]]) -> Tuple[Dict[Tuple[str, str, str], Set[str]], Dict[Tuple[str, str, str], Set[str]], Dict[Tuple[str, str, str], Set[str]], Dict[Tuple[str, str, str], List[Dict[str, str]]]]:\n    \"\"\"テーブル単位の補助情報をインデックス化（パーティション列 / クラスタ列 / PK列 / FK参照情報）\"\"\"\n    part_cols_map: Dict[Tuple[str, str, str], Set[str]] = {}\n    clus_cols_map: Dict[Tuple[str, str, str], Set[str]] = {}\n\n    for d in table_details:\n        key = (d.get('catalog_name'), d.get('schema_name'), d.get('table_name'))\n        if not all(key):\n            continue\n\n        # パーティション列\n        _, part_cols = extract_partition_info(d)\n        part_cols_map[key] = set([c.lower() for c in part_cols])\n\n        # クラスタ列（Liquid/Z-Order の列名が取れた場合のみ）\n        _, clus_cols = extract_clustering_info(d)\n        clus_cols_map[key] = set([c.lower() for c in clus_cols])\n\n    # PK/FK制約情報の取得\n    pk_cols_map, fk_refs_map = build_constraint_maps()\n\n    return part_cols_map, clus_cols_map, pk_cols_map, fk_refs_map\n\ndef build_single_column_record(column_row, part_cols_set: Set[str], clus_cols_set: Set[str], pk_cols_set: Set[str], fk_refs: List[Dict[str, str]], now_utc: str) -> Dict[str, Any]:\n    \"\"\"1つのカラムのCOLUMN_DDL_INFOレコードを構築\"\"\"\n    comment = getattr(column_row, \"comment\", None) or getattr(column_row, \"column_comment\", None)\n    col_name_lower = column_row.column_name.lower()\n    \n    # FK参照情報をこのカラムでフィルタ\n    col_fk_refs = [fk for fk in fk_refs if fk['column_name'].lower() == col_name_lower]\n    fk_reference = None\n    if col_fk_refs:\n        # 複数FK参照がある場合は最初の1つを使用\n        fk_ref = col_fk_refs[0]\n        fk_reference = f\"{fk_ref['referenced_table']}.{fk_ref['referenced_column']}\"\n\n    rec = {\n        # --- キー ---\n        \"catalog_name\":  column_row.table_catalog,\n        \"schema_name\":   column_row.table_schema,\n        \"table_name\":    column_row.table_name,\n        \"column_name\":   column_row.column_name,\n        \"ordinal_position\": int(column_row.ordinal_position),\n\n        # --- データ型 ---\n        \"data_type\":          column_row.data_type,\n        \"numeric_precision\":  column_row.numeric_precision,\n        \"numeric_scale\":      column_row.numeric_scale,\n\n        # --- 制約 ---\n        \"is_nullable\":     (str(column_row.is_nullable).upper() == \"YES\"),\n        \"default_value\":   column_row.column_default,\n        \"column_comment\":  comment,\n\n        # --- 役割フラグ ---\n        \"is_partition_column\": col_name_lower in part_cols_set,\n        \"is_clustering_column\": col_name_lower in clus_cols_set,\n        \"is_primary_key\": col_name_lower in pk_cols_set,\n        \"foreign_key_reference\": fk_reference,\n\n        # --- セキュリティ/ビジネス定義 ---\n        \"pii_classification\": \"NONE\",\n        \"encryption_required\": False,\n        \"masking_rule\": None,\n        \"column_permissions\": None,\n        \"business_description\": None,\n        \"business_rules\": None,\n        \"sample_values\": None,\n        \"quality_rules\": None,\n        \"expected_null_rate\": None,\n        \"domain_values\": None,\n        \"foreign_key_table\": None,\n        \"foreign_key_column\": None,\n        \"lookup_table\": None,\n        \"calculation_logic\": None,\n        \"source_columns\": None,\n        \"deprecated_flag\": False,\n        \"deprecation_date\": None,\n        \"replacement_column\": None,\n\n        # --- メタ ---\n        \"extracted_at\": now_utc,\n        \"extraction_method\": \"detail_python\",\n    }\n\n    return rec\n\ndef build_column_ddl_info_detail(table_details: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"COLUMN_DDL_INFO構築のメイン処理（分割後）\"\"\"\n    # 1. テーブル単位の補助情報をインデックス化\n    part_cols_map, clus_cols_map, pk_cols_map, fk_refs_map = build_column_role_maps(table_details)\n\n    # 2. 列のベース情報を取得\n    cols_df = spark.sql(\"\"\"\n        SELECT *\n        FROM base_columns\n        ORDER BY table_catalog, table_schema, table_name, ordinal_position\n    \"\"\")\n    rows = cols_df.collect()\n\n    # 3. 各カラムのレコード構築\n    out: List[Dict[str, Any]] = []\n    now_utc = datetime.now(timezone.utc).isoformat()\n\n    for r in rows:\n        key = (r.table_catalog, r.table_schema, r.table_name)\n        pset = part_cols_map.get(key, set())\n        cset = clus_cols_map.get(key, set())\n        pkset = pk_cols_map.get(key, set())\n        fk_refs = fk_refs_map.get(key, [])\n        \n        rec = build_single_column_record(r, pset, cset, pkset, fk_refs, now_utc)\n        out.append(rec)\n\n    return out"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "045938dd-e577-4dd2-a0f9-1265262efe70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 実行\n",
    "table_ddl_data = build_table_ddl_info_detail(table_details)\n",
    "print(f\"TABLE_DDL_INFO構築完了: {len(table_ddl_data)}件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64701710-d623-4081-95c3-5e2182345775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# =========================\n# TABLE_DDL_INFO → DataFrame\n# =========================\nif table_ddl_data:\n    table_schema = StructType([\n        StructField(\"catalog_name\", StringType(), False),\n        StructField(\"schema_name\",  StringType(), False),\n        StructField(\"table_name\",   StringType(), False),\n        StructField(\"table_type\",   StringType(), False),\n\n        StructField(\"storage_format\",   StringType(), True),\n        StructField(\"storage_location\", StringType(), True),\n        StructField(\"external_location\", StringType(), True),\n\n        StructField(\"partition_strategy\", StringType(), False),            # 'BY_COLUMNS' / 'NONE'\n        StructField(\"partition_columns\",  ArrayType(StringType()), True),\n\n        StructField(\"clustering_strategy\", StringType(), False),           # 'LIQUID' / 'ZORDER' / 'NONE'\n        StructField(\"clustering_columns\",  ArrayType(StringType()), True),\n\n        StructField(\"auto_optimize_write\",   BooleanType(), False),\n        StructField(\"auto_optimize_compact\", BooleanType(), False),\n        StructField(\"vacuum_retention_hours\", IntegerType(), True),\n        StructField(\"stats_column_limit\",     IntegerType(), True),\n        StructField(\"stats_custom_columns\",   ArrayType(StringType()), True),\n\n        StructField(\"cdf_enabled\",                 BooleanType(), False),\n        StructField(\"time_travel_retention_days\",  IntegerType(), True),\n\n        StructField(\"num_files\",     LongType(), True),\n        StructField(\"size_in_bytes\", LongType(), True),\n        StructField(\"size_pretty\",   StringType(), True),\n        StructField(\"table_features\", ArrayType(StringType()), True),\n\n        StructField(\"table_id\",     StringType(), True),\n        StructField(\"created_at\",   StringType(), True),   # 必要なら TimestampType に変更して後で to_timestamp\n        StructField(\"last_altered\", StringType(), True),\n        StructField(\"extracted_at\", StringType(), False),\n        StructField(\"extraction_method\", StringType(), True),\n\n        StructField(\"detail_error\", StringType(), True),   # 失敗痕跡がある場合\n    ])\n\n    table_rows = []\n    for r in table_ddl_data:\n        row = (\n            r['catalog_name'],\n            r['schema_name'],\n            r['table_name'],\n            r['table_type'],\n\n            r.get('storage_format'),\n            r.get('storage_location'),\n            r.get('external_location'),\n\n            r['partition_strategy'],\n            r.get('partition_columns') or [],\n\n            r['clustering_strategy'],\n            r.get('clustering_columns') or [],\n\n            bool(r.get('auto_optimize_write', False)),\n            bool(r.get('auto_optimize_compact', False)),\n            r.get('vacuum_retention_hours'),\n            r.get('stats_column_limit'),\n            r.get('stats_custom_columns') or [],\n\n            bool(r.get('cdf_enabled', False)),\n            r.get('time_travel_retention_days'),\n\n            r.get('num_files'),\n            r.get('size_in_bytes'),\n            r.get('size_pretty'),\n            r.get('table_features') or [],\n\n            r.get('table_id'),\n            r.get('created_at'),\n            r.get('last_altered'),\n            r['extracted_at'],\n            r.get('extraction_method'),\n\n            r.get('detail_error')\n        )\n        table_rows.append(row)\n\n    table_ddl_df = spark.createDataFrame(table_rows, table_schema)\n    table_ddl_df.createOrReplaceTempView(\"table_ddl_info\")\n    print(f\"DataFrame作成完了: {table_ddl_df.count()} レコード\")\n    display(table_ddl_df.limit(5))\n\n# ==========================\n# COLUMN_DDL_INFO → DataFrame （PK/FK情報追加版）\n# ==========================\nif column_ddl_data:\n    column_schema = StructType([\n        StructField(\"catalog_name\",  StringType(), False),\n        StructField(\"schema_name\",   StringType(), False),\n        StructField(\"table_name\",    StringType(), False),\n        StructField(\"column_name\",   StringType(), False),\n        StructField(\"ordinal_position\", IntegerType(), False),\n\n        StructField(\"data_type\",     StringType(), True),\n        StructField(\"numeric_precision\", IntegerType(), True),\n        StructField(\"numeric_scale\",     IntegerType(), True),\n\n        StructField(\"is_nullable\",   BooleanType(), False),\n        StructField(\"default_value\", StringType(), True),\n        StructField(\"column_comment\", StringType(), True),\n\n        StructField(\"is_partition_column\",  BooleanType(), False),\n        StructField(\"is_clustering_column\", BooleanType(), False),\n        StructField(\"is_primary_key\",      BooleanType(), False),    # 新規追加\n        StructField(\"foreign_key_reference\", StringType(), True),   # 新規追加\n\n        StructField(\"extracted_at\",      StringType(), False),\n        StructField(\"extraction_method\", StringType(), True),\n    ])\n\n    column_rows = []\n    for r in column_ddl_data:\n        row = (\n            r['catalog_name'],\n            r['schema_name'],\n            r['table_name'],\n            r['column_name'],\n            int(r['ordinal_position']),\n\n            r.get('data_type'),\n            r.get('numeric_precision'),\n            r.get('numeric_scale'),\n\n            bool(r['is_nullable']),\n            r.get('default_value'),\n            r.get('column_comment'),\n\n            bool(r.get('is_partition_column', False)),\n            bool(r.get('is_clustering_column', False)),\n            bool(r.get('is_primary_key', False)),        # PK情報\n            r.get('foreign_key_reference'),              # FK参照情報\n\n            r['extracted_at'],\n            r.get('extraction_method', 'detail_python'),\n        )\n        column_rows.append(row)\n\n    column_ddl_df = spark.createDataFrame(column_rows, column_schema)\n    column_ddl_df.createOrReplaceTempView(\"column_ddl_info\")\n    print(f\"DataFrame作成完了: {column_ddl_df.count()} レコード\")\n    display(column_ddl_df.limit(10))"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "319e26cc-a79e-447d-8062-4dae3979ab45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 実行\n",
    "column_ddl_data = build_column_ddl_info_detail(table_details)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================\n# PK/FK制約統計の表示\n# ============================\nprint(\"=== PK/FK制約統計 ===\")\n\n# 1. PRIMARY KEY統計\nprint(\"\\n1. PRIMARY KEY制約統計\")\nspark.sql(\"\"\"\n    SELECT \n      constraint_catalog, constraint_schema,\n      COUNT(*) AS pk_constraint_count,\n      COUNT(DISTINCT table_name) AS tables_with_pk\n    FROM pk_constraints\n    GROUP BY constraint_catalog, constraint_schema\n    ORDER BY constraint_catalog, constraint_schema\n\"\"\").show()\n\n# 2. FOREIGN KEY統計\nprint(\"\\n2. FOREIGN KEY制約統計\")\nspark.sql(\"\"\"\n    SELECT \n      constraint_catalog, constraint_schema,\n      COUNT(*) AS fk_constraint_count,\n      COUNT(DISTINCT table_name) AS tables_with_fk,\n      COUNT(DISTINCT constraint_name) AS fk_relationships\n    FROM fk_constraints\n    GROUP BY constraint_catalog, constraint_schema\n    ORDER BY constraint_catalog, constraint_schema\n\"\"\").show()\n\n# 3. カラム統計（PK/FK情報含む）\nif spark.catalog.tableExists(\"column_ddl_info\"):\n    print(\"\\n3. カラム詳細統計（PK/FK含む）\")\n    spark.sql(\"\"\"\n        SELECT\n          catalog_name, schema_name,\n          COUNT(*) AS total_columns,\n          SUM(CASE WHEN is_partition_column THEN 1 END) AS partition_columns,\n          SUM(CASE WHEN is_clustering_column THEN 1 END) AS clustering_columns,\n          SUM(CASE WHEN is_primary_key THEN 1 END) AS primary_key_columns,\n          SUM(CASE WHEN foreign_key_reference IS NOT NULL THEN 1 END) AS foreign_key_columns,\n          SUM(CASE WHEN NOT is_nullable THEN 1 END) AS not_null_columns\n        FROM column_ddl_info\n        GROUP BY catalog_name, schema_name\n        ORDER BY catalog_name, schema_name\n    \"\"\").show()\n\n# 4. FK参照関係の詳細（TOP10）\nprint(\"\\n4. FK参照関係詳細（TOP10）\")\nspark.sql(\"\"\"\n    SELECT\n      concat(catalog_name,'.',schema_name,'.',table_name,'.',column_name) AS source_column,\n      foreign_key_reference AS target_reference\n    FROM column_ddl_info\n    WHERE foreign_key_reference IS NOT NULL\n    ORDER BY catalog_name, schema_name, table_name, column_name\n    LIMIT 10\n\"\"\").show(truncate=False)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb566b77-8b51-40e8-a67c-d538b38c7a7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# TABLE_DDL_INFO → DataFrame\n",
    "# =========================\n",
    "if table_ddl_data:\n",
    "    table_schema = StructType([\n",
    "        StructField(\"catalog_name\", StringType(), False),\n",
    "        StructField(\"schema_name\",  StringType(), False),\n",
    "        StructField(\"table_name\",   StringType(), False),\n",
    "        StructField(\"table_type\",   StringType(), False),\n",
    "\n",
    "        StructField(\"storage_format\",   StringType(), True),\n",
    "        StructField(\"storage_location\", StringType(), True),\n",
    "        StructField(\"external_location\", StringType(), True),\n",
    "\n",
    "        StructField(\"partition_strategy\", StringType(), False),            # 'BY_COLUMNS' / 'NONE'\n",
    "        StructField(\"partition_columns\",  ArrayType(StringType()), True),\n",
    "\n",
    "        StructField(\"clustering_strategy\", StringType(), False),           # 'LIQUID' / 'ZORDER' / 'NONE'\n",
    "        StructField(\"clustering_columns\",  ArrayType(StringType()), True),\n",
    "\n",
    "        StructField(\"auto_optimize_write\",   BooleanType(), False),\n",
    "        StructField(\"auto_optimize_compact\", BooleanType(), False),\n",
    "        StructField(\"vacuum_retention_hours\", IntegerType(), True),\n",
    "        StructField(\"stats_column_limit\",     IntegerType(), True),\n",
    "        StructField(\"stats_custom_columns\",   ArrayType(StringType()), True),\n",
    "\n",
    "        StructField(\"cdf_enabled\",                 BooleanType(), False),\n",
    "        StructField(\"time_travel_retention_days\",  IntegerType(), True),\n",
    "\n",
    "        StructField(\"num_files\",     LongType(), True),\n",
    "        StructField(\"size_in_bytes\", LongType(), True),\n",
    "        StructField(\"size_pretty\",   StringType(), True),\n",
    "        StructField(\"table_features\", ArrayType(StringType()), True),\n",
    "\n",
    "        StructField(\"table_id\",     StringType(), True),\n",
    "        StructField(\"created_at\",   StringType(), True),   # 必要なら TimestampType に変更して後で to_timestamp\n",
    "        StructField(\"last_altered\", StringType(), True),\n",
    "        StructField(\"extracted_at\", StringType(), False),\n",
    "        StructField(\"extraction_method\", StringType(), True),\n",
    "\n",
    "        StructField(\"detail_error\", StringType(), True),   # 失敗痕跡がある場合\n",
    "    ])\n",
    "\n",
    "    table_rows = []\n",
    "    for r in table_ddl_data:\n",
    "        row = (\n",
    "            r['catalog_name'],\n",
    "            r['schema_name'],\n",
    "            r['table_name'],\n",
    "            r['table_type'],\n",
    "\n",
    "            r.get('storage_format'),\n",
    "            r.get('storage_location'),\n",
    "            r.get('external_location'),\n",
    "\n",
    "            r['partition_strategy'],\n",
    "            r.get('partition_columns') or [],\n",
    "\n",
    "            r['clustering_strategy'],\n",
    "            r.get('clustering_columns') or [],\n",
    "\n",
    "            bool(r.get('auto_optimize_write', False)),\n",
    "            bool(r.get('auto_optimize_compact', False)),\n",
    "            r.get('vacuum_retention_hours'),\n",
    "            r.get('stats_column_limit'),\n",
    "            r.get('stats_custom_columns') or [],\n",
    "\n",
    "            bool(r.get('cdf_enabled', False)),\n",
    "            r.get('time_travel_retention_days'),\n",
    "\n",
    "            r.get('num_files'),\n",
    "            r.get('size_in_bytes'),\n",
    "            r.get('size_pretty'),\n",
    "            r.get('table_features') or [],\n",
    "\n",
    "            r.get('table_id'),\n",
    "            r.get('created_at'),\n",
    "            r.get('last_altered'),\n",
    "            r['extracted_at'],\n",
    "            r.get('extraction_method'),\n",
    "\n",
    "            r.get('detail_error')\n",
    "        )\n",
    "        table_rows.append(row)\n",
    "\n",
    "    table_ddl_df = spark.createDataFrame(table_rows, table_schema)\n",
    "    table_ddl_df.createOrReplaceTempView(\"table_ddl_info\")\n",
    "    print(f\"DataFrame作成完了: {table_ddl_df.count()} レコード\")\n",
    "    display(table_ddl_df.limit(5))\n",
    "\n",
    "# ==========================\n",
    "# COLUMN_DDL_INFO → DataFrame\n",
    "# ==========================\n",
    "if column_ddl_data:\n",
    "    column_schema = StructType([\n",
    "        StructField(\"catalog_name\",  StringType(), False),\n",
    "        StructField(\"schema_name\",   StringType(), False),\n",
    "        StructField(\"table_name\",    StringType(), False),\n",
    "        StructField(\"column_name\",   StringType(), False),\n",
    "        StructField(\"ordinal_position\", IntegerType(), False),\n",
    "\n",
    "        StructField(\"data_type\",     StringType(), True),\n",
    "        StructField(\"numeric_precision\", IntegerType(), True),\n",
    "        StructField(\"numeric_scale\",     IntegerType(), True),\n",
    "\n",
    "        StructField(\"is_nullable\",   BooleanType(), False),\n",
    "        StructField(\"default_value\", StringType(), True),\n",
    "        StructField(\"column_comment\", StringType(), True),\n",
    "\n",
    "        StructField(\"is_partition_column\",  BooleanType(), False),\n",
    "        StructField(\"is_clustering_column\", BooleanType(), False),\n",
    "\n",
    "        StructField(\"extracted_at\",      StringType(), False),\n",
    "        StructField(\"extraction_method\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "    column_rows = []\n",
    "    for r in column_ddl_data:\n",
    "        row = (\n",
    "            r['catalog_name'],\n",
    "            r['schema_name'],\n",
    "            r['table_name'],\n",
    "            r['column_name'],\n",
    "            int(r['ordinal_position']),\n",
    "\n",
    "            r.get('data_type'),\n",
    "            r.get('numeric_precision'),\n",
    "            r.get('numeric_scale'),\n",
    "\n",
    "            bool(r['is_nullable']),\n",
    "            r.get('default_value'),\n",
    "            r.get('column_comment'),\n",
    "\n",
    "            bool(r.get('is_partition_column', False)),\n",
    "            bool(r.get('is_clustering_column', False)),\n",
    "\n",
    "            r['extracted_at'],\n",
    "            r.get('extraction_method', 'detail_python'),\n",
    "        )\n",
    "        column_rows.append(row)\n",
    "\n",
    "    column_ddl_df = spark.createDataFrame(column_rows, column_schema)\n",
    "    column_ddl_df.createOrReplaceTempView(\"column_ddl_info\")\n",
    "    print(f\"DataFrame作成完了: {column_ddl_df.count()} レコード\")\n",
    "    display(column_ddl_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "761154b8-1cc8-48f0-8f96-82c5998b7bec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 統合設定を使用したデータ保存とエクスポート\n",
    "meta_catalog = CONFIG[\"output_catalog\"]\n",
    "meta_schema  = CONFIG[\"target_catalog\"]\n",
    "\n",
    "# メタデータ保存先がない場合、作成\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS `{meta_catalog}`\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{meta_catalog}`.`{meta_schema}`\")\n",
    "\n",
    "table_df  = spark.sql(\"SELECT * FROM table_ddl_info\")\n",
    "column_df = spark.sql(\"SELECT * FROM column_ddl_info\")\n",
    "\n",
    "# スナップショット時刻を付与（任意）\n",
    "snap_ts = datetime.now(timezone.utc)\n",
    "table_df  = table_df.withColumn(\"snapshot_at\", F.lit(snap_ts))\n",
    "column_df = column_df.withColumn(\"snapshot_at\", F.lit(snap_ts))\n",
    "\n",
    "# 最新版テーブルとして上書き保存（Managed Delta）\n",
    "(table_df\n",
    " .write.mode(\"overwrite\")\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .saveAsTable(f\"`{meta_catalog}`.`{meta_schema}`.`{meta_schema}_table_ddl_info`\"))\n",
    "\n",
    "(column_df\n",
    " .write.mode(\"overwrite\")\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .saveAsTable(f\"`{meta_catalog}`.`{meta_schema}`.`{meta_schema}_column_ddl_info`\"))\n",
    "\n",
    "# 保持期間設定\n",
    "retention_days = CONFIG[\"retention_days\"]\n",
    "\n",
    "spark.sql(f\"\"\"  \n",
    "        ALTER TABLE `{meta_catalog}`.`{meta_schema}`.`{meta_schema}_table_ddl_info`  SET TBLPROPERTIES\n",
    "        ('delta.logRetentionDuration'='interval {retention_days} days',\n",
    "        'delta.deletedFileRetentionDuration'='interval {retention_days} days')\n",
    "        \"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "        ALTER TABLE `{meta_catalog}`.`{meta_schema}`.`{meta_schema}_column_ddl_info` SET TBLPROPERTIES\n",
    "        ('delta.logRetentionDuration'='interval {retention_days} days',\n",
    "        'delta.deletedFileRetentionDuration'='interval {retention_days} days')\n",
    "        \"\"\")\n",
    "\n",
    "print(f\"メタデータ保存完了: {meta_catalog}.{meta_schema}.*_ddl_info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48921c25-3a6f-457c-a648-00ed20e11574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 12. 手動管理項目テンプレート生成"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "unity_catalog_metadata_extractor_refactored",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}